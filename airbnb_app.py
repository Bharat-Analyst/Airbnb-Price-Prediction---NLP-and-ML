# -*- coding: utf-8 -*-
"""Airbnb_App.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/183TXCp-kk7AhwPDb7m1zEGNko0hwWvl3
"""

pip install xgboost

import numpy as np
import pandas as pd
import seaborn as sns
import sklearn

from sklearn.preprocessing import  LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn import metrics
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
#from catboost import CatBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor

data_loc = 'https://drive.google.com/uc?export=download&id=1bkcfxUZ9mH03r0yVZsS0ZRNpetzcbXA6'
df = pd.read_csv(data_loc)
df.head()

df.columns

df.isnull().sum()

for i in df.columns:
    print(i)
    print(df[i].value_counts(),"/n")
    print('--------------')

df['description'].isnull().sum() #we will use to perform NLP and get price just based of the description

"""# Handling Missing Values"""

for column in df.columns:
    print("===============================")
    print("\n{} :- {},  dtypes : {}".format(column,df[column].isnull().sum(),df[column].dtypes))

#last review, First R
print("last_review")
df.last_review.fillna(method="ffill",inplace=True)
print(df.last_review.isnull().sum())
print("first_review")
df.first_review.fillna(method="ffill",inplace=True)
print(df.first_review.isnull().sum())
print("host_since")
df.host_since.fillna(method="ffill",inplace=True)
print(df.host_since.isnull().sum())

#bathroom
sns.histplot(df["bathrooms"])

df["bathrooms"] = df['bathrooms'].fillna(round(df["bathrooms"].median()))

#Review
sns.histplot(df["review_scores_rating"])

df["review_scores_rating"] = df["review_scores_rating"].fillna(0)

#bedroom
sns.histplot(df["bedrooms"])

df["bedrooms"] = df['bedrooms'].fillna((df["bathrooms"].median()))

#bed
df["beds"] = df["beds"].fillna((df["bathrooms"].median()))
#Ametites
amenities_count = []
for i in df["amenities"]:
    amenities_count.append(len(i))

df["amenities"] = amenities_count

df.dtypes

categorical_col = []
numerical_col = []
for column in df.columns:
    if df[column].dtypes != "float64" and df[column].dtypes != "int64":
        categorical_col.append(column)
    else:
        numerical_col.append(column)

print(categorical_col)
print("-------")
print(numerical_col)

# handling categorical data

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in categorical_col:
    df[col] = le.fit_transform(df[col])
df

x = df.drop(["id","name","log_price","description","first_review","host_since","last_review","neighbourhood",
            "thumbnail_url", "zipcode"],axis = 1)
y = df.log_price

print(x)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=101)

"""# Linear Regression"""

lr = LinearRegression()
lr.fit(x_train,y_train)
y_pred_lr = lr.predict(x_test)
mae_lr = metrics.mean_absolute_error(y_test, y_pred_lr)
mse_lr = metrics.mean_squared_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_lr))
r2_lr = metrics.r2_score(y_test, y_pred_lr)
print('\nMean Absolute Error of Linear Regression     : ', mae_lr)
print('\nMean Squarred Error of Linear Regression     : ', mse_lr)
print('\nRoot Mean Squarred Error of Linear Regression: ', rmse_lr)
print('\nR2 Score of Linear Regression                : ', r2_lr)

"""# Random Forest"""

rf = RandomForestRegressor()
rf.fit(x_train,y_train)
y_pred_rf = rf.predict(x_test)
mae_rf = metrics.mean_absolute_error(y_test, y_pred_rf)
mse_rf = metrics.mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf))
r2_rf = metrics.r2_score(y_test, y_pred_rf)


print('\nMean Absolute Error of Random Forest Regressor     : ', mae_rf)
print('\nMean Squarred Error of Random Forest Regressor     : ', mse_rf)
print('\nRoot Mean Squarred Error of Random Forest Regressor: ', rmse_rf)
print('\nR2 Score of Random Forest Regressor                : ', r2_rf)

"""

```
# This is formatted as code
```

# XGBoost"""

xgb = XGBRegressor(objective="reg:squarederror")
xgb.fit(x_train,y_train)
y_pred_xgb = xgb.predict(x_test)

mae_xgb  = metrics.mean_absolute_error(y_test, y_pred_xgb)
mse_xgb  = metrics.mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb))
r2_xgb   = metrics.r2_score(y_test, y_pred_xgb)


print('\nMean Absolute Error of XGBoost Regressor     : ', mae_xgb)
print('\nMean Squarred Error of XGBoost Regressor     : ', mse_xgb)
print('\nRoot Mean Squarred Error of XGBoost Regressor: ', rmse_xgb)
print('\nR2 Score of XGBoost Regressor                : ', r2_xgb)

"""# XgBoost with Parameteres"""

# prompt: How to test different paramteres in Xgboost for above code

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Initialize XGBoost Regressor
xgb = XGBRegressor(objective="reg:squarederror")

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=5, verbose=2)

# Fit the model
grid_search.fit(x_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Get the best estimator
best_xgb = grid_search.best_estimator_

# Make predictions using the best model
y_pred_best_xgb = best_xgb.predict(x_test)

# Evaluate the best model
mae_best_xgb  = metrics.mean_absolute_error(y_test, y_pred_best_xgb)
mse_best_xgb  = metrics.mean_squared_error(y_test, y_pred_best_xgb)
rmse_best_xgb = np.sqrt(metrics.mean_squared_error(y_test, y_pred_best_xgb))
r2_best_xgb   = metrics.r2_score(y_test, y_pred_best_xgb)

print('\nMean Absolute Error of Best XGBoost Regressor     : ', mae_best_xgb)
print('\nMean Squarred Error of Best XGBoost Regressor     : ', mse_best_xgb)
print('\nRoot Mean Squarred Error of Best XGBoost Regressor: ', rmse_best_xgb)
print('\nR2 Score of Best XGBoost Regressor                : ', r2_best_xgb)

"""# Textual Data to Predict Price"""

df_text = pd.read_csv("https://drive.google.com/uc?export=download&id=1bkcfxUZ9mH03r0yVZsS0ZRNpetzcbXA6")
df_text.head()

"""# BERT Tranformer

"""

from transformers import AutoModel , BertTokenizerFast, BertModel
import torch

x = df_text["description"].apply(lambda x: len(x.split()))
x.describe()

from transformers import AutoModel , BertTokenizerFast

model = AutoModel.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

example_1 = df_text["description"][0]

output = tokenizer(example_1, return_tensors="pt")
print(output)

embedding = model(**output)
print(embedding.last_hidden_state)

"""GET BERT EMBEDDINGS AND APPEND IT TO THE DATASET FOR PREDICTION FOR DESCRIPTION COLUMN"""

from tqdm import tqdm
import torch

def get_cls_embeddings_batch(texts, model, tokenizer, max_length=512):
    inputs = tokenizer(texts, return_tensors="pt", max_length=max_length, truncation=True, padding="max_length")
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()
    return cls_embeddings

texts = df_text['description'].tolist()

batch_size = 32
embeddings = []

for i in tqdm(range(0, len(texts), batch_size)):
    batch_texts = texts[i:i+batch_size]
    batch_embeddings = get_cls_embeddings_batch(batch_texts, model, tokenizer)
    embeddings.extend(batch_embeddings)

df['embeddings'] = embeddings

print(df['embeddings'])